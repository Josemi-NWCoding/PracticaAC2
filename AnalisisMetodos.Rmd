---
title: "AnalisisMetodos"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Multi-Layer Perceptron with weight decay (mlpWeightDecay)

El primer método a analizar generará un Perceptrón Multicapa con "weight decay". Se trata de una red de tipo feed-forward con varias capas de nodos, donde los nodos de cada capa están conectados con todos los de las capas anterior y posterior. El entrenamiento de la red se realizará, en este caso, con backpropagation modificada para incluir "weight decay", lo que dificultará el sobreentrenamiento.

```{r }
library(caret)

mlpwdinfo = getModelInfo("mlpWeightDecay")
names(mlpwdinfo)
```

Podemos ver que se incluyen dos algoritmos, elegiremos el que nos interesa y a continuación veremos sus parámetros.

```{r }
mlpwdinfo = mlpwdinfo$mlpWeightDecay
mlpwdinfo$parameters
```

Los parámetros disponibles para el algoritmo son el tamaño de la red neuronal (size) y la velocidad a la que los pesos de los arcos irán disminuyendo (decay). Un tamaño demasiado pequeño no permitirá a la red alcanzar una precisión muy alta con los valores de entrenamiento ni de test, mientras que si el tamaño es demasiado alto habrá más posibilidades de que el modelo caiga en el sobreentrenamiento. Por otro lado, un decay demasiado bajo no afectará demasiado al modelo y hará que sea más fácil tener overfitting, mientras que si el decay es muy alto la red no conseguirá aprender nada, pues sus pesos bajarán demasiado rápido de valor.

Para el ajuste de los hiperparámetros usaremos la propia función grid predefinida, con un tamaño inicial de 3.

```{r }
gridmlpwd = mlpwdinfo$grid(x=NULL,y=NULL,len=3)
gridmlpwd
```


## Naive Bayes (naive_bayes)

Otro algoritmo que analizaremos será el de Naive Bayes. Es un método que genera un modelo probabilístico de las distintas hipótesis planteadas sobre un conjunto de predictores, tomando los valores de los atributos como condicionalmente independientes y facilitando así el cálculo de las probabilidades por el Teorema de Bayes de probabilidad condicionada.

```{r }
nbayesinfo = getModelInfo("naive_bayes")
names(nbayesinfo)
```

```{r }
nbayesinfo = nbayesinfo$naive_bayes
nbayesinfo$parameters
```

Este algoritmo cuenta con tres parámetros. El primero, "laplace", indica el valor utilizado en el suavizado de Laplace, una técnica que ayuda a resolver el problema que tiene Naive Bayes con la probabilidad 0. Un valor mayor de este parámetro hará que la distribución de probabilidad se aproxime más a una distribución uniforme.
El segundo parámetro, "usekernel", si tiene un valor verdadero utilizará la densidad para estimar las densidades condicionales de los predictores y sus clases.
El tercer y último parámetro, "adjust", se encargará de ajustar el ancho de banda del algoritmo.

A la hora de generar un grid para el ajuste de los hiperparámetros, vemos que el método por defecto sólo varía el parámetro de "usekernel", dejando "laplace" a 0 y "adjust" a 1.

```{r }
nbayesinfo$grid
```

Es por esto que definiremos nuestra propia grid, para probar también combinaciones de dichos parámetros.

```{r }
#Sacada la info sobre parámetros de:
#https://cran.r-project.org/web/packages/naivebayes/naivebayes.pdf
#https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece

gridnbayes = expand.grid(usekernel = c(TRUE, FALSE), laplace = c(0, 1, 10), adjust = c(0.5, 1, 1.5))
gridnbayes
```

La elección de los valores de "laplace" se debe a que con 0 no se aplica el suavizado de Laplace, el 1 es un valor suficiente para que el suavizado sea notable y el 10 es un valor superior para asegurar que no descuidemos una posible mejora al aumentar el parámetro. Respecto a los valores de "adjust", simplemente utilizamos el valor por defecto de 1 como valor central, y probamos los casos para 0.5 por debajo y por encima de dicho valor.

## Árboles CART (rpart)

A continuación analizaremos el algoritmo rpart, que nos generará un árbol CART a partir de los datos de entrenamiento. El algoritmo utiliza un criterio de selección del split de nodos basado en ascensión de colinas y una poda basada en equilibrar el coste y la complejidad del árbol, intentando evitar el overfitting (demasiada complejidad) y el underfitting (complejidad demasiado baja).

```{r }
rpartinfo = getModelInfo("rpart")
names(rpartinfo)
```

Entre los distintos paquetes disponibles elegiremos el indicado, "rpart".

```{r }
rpartinfo = rpartinfo$rpart
rpartinfo$parameters
```

Podemos ver que sólo dispone de un parámetro, "cp", que corresponde al parámetro de complejidad que determinará a qué nivel de complejidad se podarán ramas del árbol, pudiendo ajustarlo para conseguir árboles con mayor o menor complejidad.

No obstante, nos encontramos con que no podemos utilizar este método para solucionar nuestro problema. Como se ha visto en el tema 5 de teoría, los árboles CART son específicos para problemas de regresión, mientras que el problema con el que debemos lidiar es de clasificación. Es por esto que debemos desechar este método.

## Bagged AdaBoost (AdaBag)

Es un algoritmo de clasificación basado en ensamblajes, concretamente en bagging y en boosting. El algoritmo combina un algoritmo de bagging para generar predictores combinados (agrupados) en base a los predictores iniciales, los almacena en forma de árbol y utiliza estos resultados en el algoritmo Adaboost, que los combina en una suma ponderada que consigue mejores resultados.

```{r }
#https://cran.r-project.org/web/packages/adabag/adabag.pdf
#https://en.wikipedia.org/wiki/AdaBoost
#https://link.springer.com/article/10.1023/A:1018054314350
```

```{r }
adabaginfo = getModelInfo("AdaBag")
names(adabaginfo)
```

```{r }
adabaginfo = adabaginfo$AdaBag
adabaginfo$parameters
```

Los parámetros de que disponemos son "mfinal" y "maxdepth". El primero controla el número de iteraciones que se ejecutará el boosting, o bien el número de árboles que se usará. El segundo, por su parte, establece el límite de profundidad que pueden tener los nodos en el árbol final generado.

Para el análisis de hiperparámetros nos bastará, en principio, con utilizar el método grid predefinido, en este caso con un tamaño de 3.

```{r }
gridadabag = adabaginfo$grid(x=NULL,y=NULL,len=3)
gridadabag
```



